# Copilot Implementation Instructions for Shared Memory Transport

## Setup

- **Environment:** Ensure you have a recent Go toolchain installed (gRPC-Go requires one of the latest two Go releases). Set up the `grpc-go` repository locally by forking and cloning it. Create a new branch for the shared memory transport feature to isolate your work.
- **Repository Structure:** Familiarize yourself with the `grpc-go` repository layout. The transport internals reside in the `internal/transport` package. Plan to implement the shared memory transport within this internal package (or a new subpackage under it) to avoid exposing unstable APIs externally.
- **Dependencies:** Avoid adding new external dependencies for this feature. If you need low-level system calls (e.g. futex or eventfd), use Go’s standard libraries or `golang.org/x/sys` as needed. Introducing new dependencies requires discussion with maintainers. 
- **License Header:** When creating new files, include the standard gRPC license header at the top (copy from an existing file and update the year).
- **Initial Test Run:** Before writing new code, run the existing test suite to ensure you have a green baseline. Use the provided scripts and commands (e.g. `./scripts/vet.sh`, `go test -cpu 1,4 -timeout 7m ./...`) to catch any issues early. All tests should pass before you proceed with new changes.

## Development Rules

- **Follow Project Conventions:** Adhere to gRPC-Go’s development guidelines at all times. We follow **Google’s Go style guide** in this project. In case of any doubt, prefer consistency with existing `grpc-go` code over personal preference. Keep your pull requests focused and incremental – implement one feature or fix at a time.
- **No OS Threads:** Do not create OS threads manually. Use Go’s goroutines for concurrency, and only spawn goroutines when necessary for this transport’s logic. Rely on Go’s scheduler to handle multiplexing; avoid any form of busy-waiting or continuous polling that could tie up a thread.
- **Event-Driven Concurrency:** **Avoid polling loops and `time.Sleep` delays.** All waiting for data or state changes should be event-driven. For example, if a reader needs to wait for data to arrive in shared memory, have it block on a synchronization primitive and get unblocked when data is available, rather than spinning in a loop. Polling wastes CPU cycles and power, whereas asynchronous notifications wake up the process only when needed.
- **Minimize Kernel Calls:** Favor user-space synchronization and communication over system calls. Every context switch or syscall is costly. Use atomic operations for counters and flags, and use advanced synchronization primitives like futexes or eventfds when cross-process coordination is needed, because they allow user-space waiting with minimal kernel involvement. For example, a futex can wait on a value in shared memory without entering the kernel unless it has to. An eventfd can be signaled and waited on with low overhead and can integrate with Go’s runtime network poller if needed.
- **Cross-Process Design:** Remember that the client and server run in **separate processes** (no shared heap, no common garbage collector). Design your shared memory region and synchronization such that **unrelated processes** can access it. This likely means using memory that is backed by a file or OS-supported shared memory object (e.g., a file in `/dev/shm` or an anonymous memfd) which both processes can open. You will need a strategy to establish this shared memory region during connection setup (for example, using a known file name or passing a file descriptor). One side must create the shared memory and the other side must open it; coordinate this via the dial/listen address or an out-of-band handshake.
- **Consistency and Portability:** The implementation should work across OS platforms or degrade gracefully. Use build tags if OS-specific system calls are required (e.g., Linux for `futex` or `eventfd`). For non-Linux systems that lack these syscalls, implement fallbacks (such as using a regular `sync.Mutex` or `sync.Cond` on a memory-mapped file) so that the code at least compiles and runs tests on those platforms. **Never assume** a single-OS environment; ensure that tests cover multiple platforms if possible. Portability and correctness take priority over squeezing out platform-specific optimizations.

## Code Style

- **General Style:** Follow Go idioms and the project's established style rigorously. This project abides by Google’s Go style guide, which in turn builds on effective Go practices. That means using `gofmt` formatting, idiomatic naming, and clear comments. **Run `gofmt` and linters on all new code** before committing to ensure compliance.
- **Naming Conventions:** Use clear, descriptive names that match project conventions. **Avoid generic names** like `util` or `common`. For package-level names, avoid stuttering (e.g., if the package is `sharedmem`, a type should be `Transport`, not `SharedMemTransport.SharedMemTransport`). Follow Go naming best practices: use MixedCaps and capitalize acronyms properly. For example, initialisms should stay consistent in case – “ID” not “Id”, “GRPC” for exported symbols containing “gRPC” vs. “gRPC” for unexported. Never use underscores in identifiers except in test function names if needed (and even then, only to improve readability).
- **Structure and Organization:** Place new code in the appropriate package and file. For instance, the shared memory transport types (client and server) should reside alongside existing transports. If the HTTP/2 transport implementation is in `internal/transport/http2_client.go` and `.../http2_server.go`, you might create `internal/transport/shm_client.go` and `shm_server.go` (or a subfolder if extensive). Keep internal helper functions unexported. Organize the code so that high-level interfaces and types are at the top of the file, and lower-level helper functions are below.
- **Comments and Documentation:** Document all exported types, functions, and any complex internal logic. Use complete sentences and follow GoDoc style – the first sentence of a comment should be a summary that starts with the name of the item. Make sure your comments explain the *why* and *how* of the approach, especially because shared memory concurrency can be tricky. This will help future maintainers and also serve as context for the AI agent itself (comments are part of the AI’s input and should be clear and accurate).
- **Go Lint and Vet:** Your code must pass all linting and vetting checks. The gRPC-Go project likely uses `golangci-lint` or similar in CI; ensure no warnings from linters like `go vet`, `staticcheck`, etc. For example, check for unchecked errors, misuse of `context`, or any race conditions flagged by the race detector. Running `./scripts/vet.sh` (provided in the repo) and `go test -race ./...` should produce **zero** errors or warnings. Adhere to any additional lint rules the project has configured (e.g., avoid using deprecated APIs, maintain consistent import ordering, etc.).

## Testing Expectations

- **Test-Driven Approach:** Treat tests as a first-class part of this development. For every functional update or bug fix, **write or update unit tests** in the same commit. Ideally, begin by writing tests that cover the expected behavior of the shared memory transport, then implement until those tests pass. This will give the AI agent immediate feedback on whether the code works as intended.
- **Continuous Testing:** After each code change, **run the tests** to verify nothing is broken. This is especially crucial when dealing with concurrency – even a small change can introduce data races or deadlocks. Automate this habit: require the AI agent to run `go test` (with `-race`) after every change. In fact, you *should* have the agent run tests after every code update; this ensures the code remains stable and that the AI is doing a good job. The tests’ passing or failing will guide the agent’s next steps.
- **Unit Tests:** Create focused unit tests for the shared memory transport logic. This might include tests for:
  - **Basic Data Exchange:** Send a message from a gRPC client to a server using the shared memory transport and verify the message is received intact.
  - **Concurrent Streams:** Simulate multiple gRPC streams/messages in parallel to ensure your transport can handle concurrency without data races or starvation.
  - **Edge Cases:** Buffer boundary conditions (when the shared buffer is almost full or empty), large message payloads, abrupt closure of one side, etc. For example, test what happens if the client process crashes or the connection is closed mid-stream.
  - **Synchronization Correctness:** You should test internal functions that manage the shared memory ring buffer (if using one). For instance, if you implement a circular buffer, have tests for wrap-around logic and proper waking of waiting readers/writers when buffer conditions change.
- **Integration Tests:** If possible, write higher-level tests that start a real gRPC server and client with your new transport (perhaps using a custom dial option or address scheme like `"shm://<name>"`). These tests can use two separate OS processes or threads. One approach is to invoke a test helper binary (via `os/exec`) to act as the server in a separate process, ensuring truly separate memory spaces. If that’s too complex, you can simulate two processes in one test process by using two separate memory regions and transports (just be careful to avoid false sharing). The goal is to verify end-to-end communication without going through the kernel network stack.
- **Performance Tests (Optional):** While correctness is the priority, you might include some benchmarks or micro-tests to measure latency and throughput of the shared memory transport versus the standard TCP loopback. This can help validate that we indeed minimize overhead. However, ensure these are not run as part of normal unit tests (use Go’s `testing.B` for benchmarks).
- **Race Detection:** Always run tests with the race detector (e.g., `go test -race -cpu 1,4 ./...`) before considering the implementation complete. The shared memory transport involves concurrency between processes and goroutines; the race detector can catch unsynchronized access in your Go code. (Note that the race detector won’t catch issues between processes by itself, so design your synchronization carefully.)
- **Expect Flaws, Iterate Quickly:** If a test fails or a new bug is found, address it in the very next iteration. The agent should use the failing test output to diagnose the issue, then adjust the code. Keep the feedback loop tight. **Do not proceed** to new features if tests are failing; fix the problem first and get back to green.

## IPC Guidelines

- **Shared Memory Region:** Implement a mechanism for creating and sharing a memory region accessible to both the client and server processes. On Linux, you can use an anonymous memory file (e.g., via `memfd_create` or `shm_open`) and `mmap` it into both processes’ address spaces. If using `memfd_create`, you get a file descriptor that can be passed or named; using `shm_open` gives a named handle in `/dev/shm`. Design a clear protocol for how the client and server agree on the shared memory handle:
  - The **server** could create the shared memory segment when it starts listening (e.g., using a name derived from the address or a randomly generated token) and then advertise that to clients (perhaps via the connection address or an initial gRPC handshake metadata).
  - The **client** upon dialing could either create (for client-initiated segments) or open an existing shared memory segment based on the target address. Ensure error handling for cases where the segment isn’t ready or cannot be opened.
  - Clean up the shared memory when the transport is closed (unlink the `shm` file or close the memfd) to avoid leaks.
- **Memory Layout:** Decide on a memory layout for data exchange. A common approach is a ring buffer or dual ring buffers for bi-directional data. For example, allocate one circular buffer for data from client->server and another for server->client, or a single shared region partitioned in half for each direction. Store write and read indices (offsets) in shared memory as well. Use `atomic` operations to update these indices safely from each side. Align these indices to 32-bit or 64-bit boundaries as needed for atomic access.
- **Synchronization Primitives:** Use futexes or eventfd to synchronize when one side needs to wait for the other:
  - **Futex:** A futex is essentially a 32-bit integer in shared memory that threads/processes can wait on or wake. For example, you might have a `uint32` value that represents the available bytes in the buffer or a state flag; if a reader finds no data, it calls `futex(FUTEX_WAIT)` on that address, which will block efficiently until the writer does a `FUTEX_WAKE` after writing data. Futexes allow near zero-cost waiting when the condition is not met (user-space will check the value and only enter the kernel to sleep if needed).
  - **Eventfd:** An `eventfd` is a Linux kernel object that can be waited on (via epoll) and signaled by writing to it. You might use an eventfd for each direction as a readiness indicator (e.g., writer writes to eventfd when new data is available, reader blocks on eventfd). A key advantage is that one goroutine can wait on multiple eventfds using `epoll` (or Go’s runtime network poller) while futex only allows waiting on one address at a time. If you need to handle multiple shared memory transports in one process, eventfd integrates well with Go’s `netpoll` mechanism for scale.
  - **Choice:** You can choose either approach or even a combination. For simplicity and minimal kernel transitions, a futex on a flag (in shared memory) can be very fast for a single connection. If you envision multiple connections handled by a single goroutine, eventfds might be easier to manage in a `select/epoll` loop. **Do not use busy loops** to wait for data; always block on one of these primitives to yield the CPU.
- **Zero-Copy Data Transfer:** One goal of a shared memory transport is to avoid copying bytes between kernel and user space. Ensure that once a message is serialized into bytes (via protobuf), you copy it into the shared memory buffer directly and the other side reads it out from the buffer. Avoid intermediate copies. Use efficient methods to copy from `[]byte` to the memory map (for example, `copy()` in Go can move data into a `[]byte` that’s backed by mmap). If using multiple buffers or segments, consider memory alignment and padding for cache efficiency, but **do not prematurely optimize** by doing anything complex that sacrifices clarity or portability.
- **Protocol and Framing:** The transport must carry gRPC frames (length-prefixed messages, as per HTTP/2 framing for gRPC). You might be able to reuse some of gRPC-Go’s framing logic. Ensure that your shared memory transport properly delineates message boundaries:
  - You could prepend each message with its length (just like HTTP/2 does for DATA frames). Write the length as a fixed-size header in the ring buffer before the message bytes.
  - The reading side should wait until enough bytes (length + message) are available in the buffer before considering a message complete. If not enough space remains at the end of the buffer for a full message, you may need to wrap around (handle ring buffer wrap correctly) or use a marker to indicate a wrap.
  - Reuse existing proto buffer marshalling as is – this transport operates at the byte level *below* the gRPC message layer. So, you don’t need to change how protobuf encodes messages; just transport those bytes.
- **Error Handling:** Implement robust error handling and state management:
  - If one side of the transport is closed or crashes, the other side’s reads/writes should notice this (perhaps via the shared memory being unmapped or a special flag). For example, if the server closes the connection, it could set a shared flag or simply unlink the shared memory so future accesses fail – the client should then return an error like “transport closed”.
  - Handle partial reads/writes. It's possible (though with a ring buffer design you might always write fully or block) that you can’t write all data in one go if the buffer is full; in that case, block (with futex/eventfd) until space is free, or implement backpressure by signaling the higher layers to slow down.
  - Watch out for deadlocks: ensure that any waiting condition has a corresponding wake-up. Use timeouts in tests to detect deadlock scenarios.
- **Resource Cleanup:** Both client and server should release resources on termination. That means unmapping the memory (using `munmap` or letting the memory segment close when the file descriptor is closed) and closing any file descriptors (eventfds, memfd, etc.). The server should unlink any named shared memory object when it’s done serving (to prevent leaks in `/dev/shm`). Use `defer` where appropriate to guarantee cleanup.

## Performance Constraints

- **Correctness First:** Emphasize **correctness and clarity over micro-optimizations**. Shared memory IPC is complex; a subtle bug can corrupt data or crash processes, so it’s far more important to have a robust, easy-to-review implementation than to shave off a few nanoseconds. Always favor clear algorithms and well-synchronized code. Only optimize (and only in safe ways) after you have tests proving correctness. As a rule, do not apply any “clever” optimizations that sacrifice code readability or maintainability unless you have profiling evidence that it’s necessary, and even then, ensure the behavior is well-tested.
- **Avoid Premature Optimization:** Resist the temptation to implement exotic lock-free structures or hand-tuned assembly for this transport. The Go runtime and OS will handle a lot of optimization for you. Use high-level concurrency patterns (mutexes, channels, etc.) if they make the code simpler – it’s fine if those internally use futexes. A straightforward correct solution is better than an unstable optimized one. Remember the guidance: prioritize establishing a correct baseline; you can optimize later if needed.
- **Low Overhead Path:** That said, the design itself should inherently reduce overhead by avoiding kernel calls in the data path. Once the shared memory segment is set up, data writes and reads should mostly be just memory copies and atomic operations – no syscalls for each message. The only syscalls should be for synchronization when contended (e.g., futex wait if no data). This will already yield a performance benefit over socket-based transport which goes through the kernel for every packet. Ensure that your implementation indeed follows this model: for example, writing data should *not* call into the kernel except when the buffer is full and the writer needs to sleep.
- **Latency Considerations:** One goal is to minimize latency for local RPCs. Measure the critical path: when a
